# -*- coding: utf-8 -*-
"""Finetuning_T5_English_Scoring_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/197Obd0xsjAVmA_opwxQLp0XxQ0mdu-A1

# Import important Package
"""

from huggingface_hub import notebook_login
notebook_login()

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import logging
# logging.disable(logging.WARNING)
# 
# # This fixes colab's default encoding to match huggingface accelerate
# import locale
# locale.getpreferredencoding = lambda x=False: "UTF-8"
# !pip install datasets
# !pip install transformers #get transformers
# !pip install --upgrade accelerate
# !pip install evaluate
# !pip install rouge-score #to get the accuracy metric

from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification
from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling
# import datasets
# from datasets import load_dataset, list_datasets
from evaluate import evaluator
import evaluate
import numpy as np
import torch
import copy
import pandas as pd


from collections import defaultdict

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_id = 0 if str(device) == 'cuda' else -1

# Read training dataset
datasets = pd.read_csv('train.csv')

datasets

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_id="google-t5/t5-small"

# Load tokenizer of FLAN-t5-base
tokenizer = AutoTokenizer.from_pretrained(model_id)

datasets

# Read training dataset
datasets = pd.read_csv('train.csv')
#basic steps for tokenizer
datasets['tokenize'] = datasets["full_text"].apply(lambda x: tokenizer(x, max_length=512, padding='max_length', truncation=True))
datasets['input_ids'] = datasets['tokenize'].apply(lambda x: x['input_ids'])
datasets['attention_mask'] = datasets['tokenize'].apply(lambda x: x['attention_mask'])
# datasets['input_ids'] = datasets['input_ids'].apply(lambda x: [int(i) for i in x])  # Convert to int list
#est['input_ids'] = test['input_ids'].apply(lambda x: [int(i) for i in x])
# datasets['label'] = datasets[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']].astype(str).agg(','.join, axis=1)
datasets['label'] = datasets[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']].astype(float).values.tolist() #combine label

idx = round(0.8*len(datasets)) #split for train and test
train = datasets[:idx]
test = datasets[idx:]

train = train[['input_ids','attention_mask','label']]
test = test[['input_ids','attention_mask','label']]

test

train.reset_index(drop=True, inplace=True)
test.reset_index(drop=True, inplace=True)

from datasets import Dataset

train_dataset = Dataset.from_pandas(train) #turning to Dataset type object
test_dataset = Dataset.from_pandas(test)

from transformers import DataCollatorForSeq2Seq
from transformers import AutoTokenizer
from random import randrange
import torch

# Data collator for input_idx and attention_mask as a list
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model_id,
    pad_to_multiple_of=6
)

from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset

#initialize T5
t5 = T5ForConditionalGeneration.from_pretrained('t5-base')

# Define training arguments
training_args = TrainingArguments(
    evaluation_strategy="epoch",      # Evaluate at the end of each epoch
    save_strategy="epoch",            # Save at the end of each epoch to match
    logging_strategy="epoch",
    output_dir="t5-training",
    learning_rate=5e-5,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_total_limit=2,
    load_best_model_at_end=True,
)



# Compute metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):
        logits = logits[0]

    # Determine predictions
    predicted_values = np.mean(logits, axis=1) #get the average accross the array
    predicted_values = np.argmax(logits, axis=-1)  # get the max score


    # Calculate metrics
    mse = ((predicted_values - labels) ** 2).mean()
    return {'mse': mse}





# Initialize Trainer
trainer = Trainer(
    data_collator=data_collator,
    model=t5,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

test_dataset

# After training
predictions_output = trainer.predict(test_dataset)

predictions_output

import numpy as np

# get the first tuple
logits = predictions_output.predictions[0]

# Compute mean
# Get the max score
predicted_values = np.mean(logits, axis=1)
predicted_values = np.argmax(logits, axis=-1)
print(predicted_values.shape)

predicted_values #look good!!

from huggingface_hub import login

login()

trainer.push_to_hub('khanhvy31/baset5_scoring') #push to hugging face

















































































